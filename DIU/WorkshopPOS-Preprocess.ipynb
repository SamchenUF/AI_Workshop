{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d99cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b20975",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "Get a few hundred words for training and test data from the Brown Corpus via nltk. \n",
    "To imitate field data, randomly change POS tags for 20% of train set.\n",
    "Extract a few thousands words to simulate the rest of the field data which has not been annotated yet. \n",
    "\n",
    "Write the three data sets to files for printing and sharing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "347683d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random sentences from Brown corpus\n",
    "text = random.choices(list(nltk.corpus.brown.tagged_sents(tagset=\"universal\")),k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3d86c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag list: ['ADP', 'NUM', 'CONJ', 'ADJ', 'DET', 'NOUN', 'PRT', 'VERB', '.', 'ADV', 'PRON']\n",
      "Train Tokens: 308\n",
      "Untagged tokens: 5782\n",
      "Sample sentences:\n",
      " [[('Total', 'NOUN'), ('distance', 'NOUN'), ('is', 'VERB'), ('about', 'ADV'), (\"21/64''\", 'NOUN'), (\"''\", '.'), ('.', '.')], [('Over', 'ADP'), ('a', 'DET'), ('relatively', 'ADV'), ('short', 'ADJ'), ('period', 'NOUN'), ('of', 'ADP'), ('time', 'NOUN'), (',', '.'), ('usually', 'ADV'), ('about', 'ADV'), ('four', 'NUM'), ('to', 'ADP'), ('twelve', 'NUM'), ('weeks', 'NOUN'), (',', '.'), ('the', 'DET'), ('worker', 'NOUN'), ('must', 'VERB'), ('be', 'VERB'), ('able', 'ADJ'), ('to', 'PRT'), ('shift', 'VERB'), ('the', 'DET'), ('focus', 'NOUN'), (',', '.'), ('back', 'ADV'), ('and', 'CONJ'), ('forth', 'ADV'), (',', '.'), ('between', 'ADP'), ('immediate', 'ADJ'), ('external', 'ADJ'), ('stressful', 'ADJ'), ('exigencies', 'NOUN'), ('(', '.'), ('``', '.'), ('precipitating', 'VERB'), ('stress', 'NOUN'), (\"''\", '.'), (')', '.'), ('and', 'CONJ'), ('the', 'DET'), ('key', 'ADJ'), (',', '.'), ('emotionally', 'ADV'), ('relevant', 'ADJ'), ('issues', 'NOUN'), ('(', '.'), ('``', '.'), ('underlying', 'VERB'), ('problem', 'NOUN'), (\"''\", '.'), (')', '.'), ('which', 'DET'), ('are', 'VERB'), (',', '.'), ('often', 'ADV'), ('in', 'ADP'), ('a', 'DET'), ('dramatic', 'ADJ'), ('preconscious', 'ADJ'), ('breakthrough', 'NOUN'), (',', '.'), ('reactivated', 'VERB'), ('by', 'ADP'), ('the', 'DET'), ('crisis', 'NOUN'), ('situation', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('hence', 'ADV'), ('once', 'ADV'), ('again', 'ADV'), ('amenable', 'ADJ'), ('to', 'ADP'), ('resolution', 'NOUN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, test, and untagged set\n",
    "train_set = text[:10]\n",
    "test_set = text[10:35]\n",
    "untagged_set = text[35:]\n",
    "\n",
    "# List all POS tags used in the data\n",
    "tags = [tag for sent in train_set for word,tag in sent]\n",
    "tagset = list(set(tags))\n",
    "tokens = len(tags)\n",
    "\n",
    "print(\"Tag list:\", tagset) # should match nltk Universal POS Tagset\n",
    "print(\"Train Tokens:\", tokens)\n",
    "print(\"Untagged tokens:\", len([count for sent in untagged_set for count in sent]))\n",
    "print(\"Sample sentences:\\n\", train_set[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "060e1ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 66, '.': 48, 'VERB': 39, 'ADP': 39, 'DET': 36, 'ADJ': 28, 'ADV': 27, 'CONJ': 9, 'PRON': 9, 'PRT': 5, 'NUM': 2})\n"
     ]
    }
   ],
   "source": [
    "# Tag statistics\n",
    "tag_freq = Counter(tags)\n",
    "print(tag_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cce01d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADP', 'NUM', 'CONJ', 'ADJ', 'DET', 'NOUN', 'PRT', 'VERB', 'ADV', 'PRON']\n"
     ]
    }
   ],
   "source": [
    "# don't allow punctuation tag as possible mistake\n",
    "tagset.remove('.')\n",
    "print(tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb32c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly change about 25% of POS tags to create \"mistakes\"\n",
    "switch_idx = random.sample(range(0,tokens), 55)\n",
    "\n",
    "random_train_set = []\n",
    "wrd_counter = 0\n",
    "for sent in train_set:\n",
    "    random_sentence = []\n",
    "    for pair in sent:\n",
    "        wrd_counter+=1\n",
    "        if wrd_counter in switch_idx and pair[1] != '.': # if punctuation, skip switch, change tag to PUNCT\n",
    "            random_sentence.append((pair[0], random.choice(tagset)))\n",
    "        else:\n",
    "            random_sentence.append(pair)\n",
    "    random_train_set.append(random_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2aa6732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 58, '.': 48, 'DET': 39, 'ADP': 38, 'VERB': 35, 'ADJ': 28, 'ADV': 24, 'CONJ': 14, 'PRT': 10, 'PRON': 9, 'NUM': 5})\n"
     ]
    }
   ],
   "source": [
    "# Tag statistics\n",
    "newtags = [p for sent in random_train_set for w,p in sent ]\n",
    "new_tag_freq = Counter(newtags)\n",
    "print(new_tag_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ecef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataFiles(tuple_dataset, filename, tagged=True):\n",
    "    '''write data sets to files for printing and sharing\n",
    "    Remove POS tags if data is supposed to be unannotated'''\n",
    "    \n",
    "    datastring = []\n",
    "    for sent in tuple_dataset:\n",
    "        sentence = []\n",
    "        for pair in sent:\n",
    "            if tagged:\n",
    "                sentence.append(pair[0] + '/' + pair[1])\n",
    "            else: \n",
    "                sentence.append(pair[0])\n",
    "        datastring.append(' '.join(sentence))\n",
    "\n",
    "    with open(filename, 'w') as T:\n",
    "        T.write('\\n'.join(datastring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07d893f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFiles(random_train_set, 'DIU_origin0.train')\n",
    "dataFiles(test_set, 'DIU_origin0.test')\n",
    "dataFiles(untagged_set, 'DIU_origin0.predict', tagged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4da280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
