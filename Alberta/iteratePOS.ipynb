{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8eb3b99",
   "metadata": {},
   "source": [
    "# *If you are not interested in how the models work or do not plan to customize the code, jump to the <a href='#run-section'>\"Run Code\"</a> section.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d99cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import sklearn_crfsuite\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import itertools\n",
    "import datetime\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf86e815",
   "metadata": {},
   "source": [
    "# Data Prep Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(filepath):\n",
    "    pairswbar = [line.strip().split() for line in open(filepath)]\n",
    "    tuple_lines = []\n",
    "    for line in pairswbar:\n",
    "        tuple_lines.append([tuple(pair.split(SEPARATOR)) for pair in line])\n",
    "    return tuple_lines\n",
    "\n",
    "\n",
    "def inandout(tuple_lines):\n",
    "    inputlines = []\n",
    "    outputlines = []\n",
    "    for line in tuple_lines:\n",
    "        inline = []\n",
    "        outline = []\n",
    "        for word,tag in line:\n",
    "            inline.append(word)\n",
    "            outline.append(tag)\n",
    "        inputlines.append(' '.join(inline))\n",
    "        outputlines.append(' '.join(outline))\n",
    "    return inputlines, outputlines\n",
    "\n",
    "\n",
    "def datafile(filename, data):\n",
    "    with open(DATADIR+filename, 'w') as T:\n",
    "        T.write('\\n'.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720054e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logstats(train_tuples,filepath):\n",
    "    # List all POS tags used in the data\n",
    "    taglist = [tag for sent in train_tuples for word,tag in sent]\n",
    "    tagset = list(set(taglist))\n",
    "    tokens = len(train_tuples)\n",
    "    sorted_tags = sorted(Counter(taglist).items(), key=lambda x:x[1], reverse=True)\n",
    "    sort_string = ''\n",
    "    for tag,val in sorted_tags:\n",
    "        sort_string += \"{: <10}\\t{}\\n\".format(tag, val)\n",
    "    \n",
    "    logstring = str(datetime.datetime.now()) + '\\n\\n'    \n",
    "    tokenreport = str(tokens) + \" training tokens\"  + '\\n\\n'\n",
    "    tagreport = \"Tags in training data:\\n\" + sort_string  + '\\n'\n",
    "    #print(tagreport) \n",
    "    #print(tokenreport)\n",
    "    #print(tagcounts)\n",
    "    \n",
    "    msg = logstring+NOTES+tokenreport+tagreport\n",
    "    \n",
    "    with open(filepath+'_log.txt', 'w') as l:\n",
    "        l.write(filepath + '\\n\\n' + msg)\n",
    "    \n",
    "    return tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(modelname):\n",
    "    '''write data sets to files for printing and sharing'''\n",
    "    fname = TEAMCODE+TASK+modelname\n",
    "    unannotated_tuples = retrieve(DATADIR+fname+PREVITERATION+'.predict')\n",
    "    train_pairs = retrieve(DATADIR+fname+ITERATION+'.train')\n",
    "    test_pairs = retrieve(DATADIR+fname+ITERATION+'.test')\n",
    "        \n",
    "    # Log training data statistics\n",
    "    uniqtags = logstats(train_pairs, REPORTDIR+fname+ITERATION)\n",
    "        \n",
    "    return train_pairs, test_pairs, unannotated_tuples, uniqtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d2ea7",
   "metadata": {},
   "source": [
    "# Results Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6060cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm(gold_tags, predicted_tags, taglist, modelname):\n",
    "    '''builds and display a confusion matrix so we \n",
    "    can evaluate where our tagger is making wrong \n",
    "    predictions, after we test a POS tagger'''\n",
    "    \n",
    "    alpha_taglist = sorted(set(taglist))\n",
    "    confusion_matrix = metrics.confusion_matrix(gold_tags,predicted_tags,labels=alpha_taglist,normalize=\"true\")\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix,display_labels=alpha_taglist)\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (17,17)\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    disp.plot(colorbar=False)\n",
    "    plt.show() # display below\n",
    "    #save as file\n",
    "    plt.savefig(REPORTDIR+TEAMCODE+TASK+modelname+ITERATION+'_matrix.png')    \n",
    "    \n",
    "    matrixstring = '{0:5s}'.format(' ') + '\\t'.join(['{0:^4s}'.format(tag) for tag in alpha_taglist]) + '\\n'\n",
    "    for i,row in enumerate(confusion_matrix):\n",
    "        cols = '\\t'.join(['{:.2f}'.format(round(col,2)) for col in row])\n",
    "        matrixstring+='{0:6s}'.format(alpha_taglist[i]) + cols + '\\n'\n",
    "    \n",
    "    return matrixstring\n",
    "\n",
    "    \n",
    "def logResults(testgold, testpredict, confusionreport, modelname):\n",
    "    time = str(datetime.datetime.now())\n",
    "    testgold = list(itertools.chain.from_iterable(testgold))\n",
    "    testpredict = list(itertools.chain.from_iterable(testpredict))\n",
    "    classreport = metrics.classification_report(testgold, testpredict, zero_division=0.0)\n",
    "    report = '\\n\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n'.format(classreport, confusionreport)\n",
    "    with open(REPORTDIR+TEAMCODE+TASK+modelname+ITERATION+'_results.txt', 'w') as R:\n",
    "        R.write(time + report)\n",
    "        \n",
    "        \n",
    "def printPredictions(confscores, w_t_predictions, modelname):\n",
    "    '''sort auto-annotated sentences based on how \"confident\" \n",
    "    the model was at it predictions of each sentence's POS tags, \n",
    "    by decreasing \"confidence\", \n",
    "    i.e., lower probability == less confidence.\n",
    "    Writes to file.'''\n",
    "    \n",
    "    with_confidence = list(zip(w_t_predictions, confscores))\n",
    "    with_confidence.sort(key = lambda x: x[1])\n",
    "    sorted_predictions = [z[0] for z in with_confidence]\n",
    "    \n",
    "    datastring = []\n",
    "    for sent in sorted_predictions:\n",
    "        datastring.append(' '.join([pair[0]+SEPARATOR+pair[1] for pair in sent]))\n",
    "\n",
    "    datafile(TEAMCODE+TASK+modelname+ITERATION+'.predict', datastring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b0693",
   "metadata": {},
   "source": [
    "# POS Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fdf80a",
   "metadata": {},
   "source": [
    "With any model, we take the same three basic steps.\n",
    "\n",
    "1) Train the model. The models use some statistical patterns or features of the training data with its POS tags to build a predictive model. \n",
    "\n",
    "2) Use the trained model to predict POS tags over the test sentences. Then compare those predictions to the correct tags and produce evaluation metrics for that model. We use precision, recall, F1 scores, and a confusion matrix.\n",
    "\n",
    "3)  Predict tags over our unannotated sentences. Then sort these sentences based on the model's \"confidence\" in those predictions. That is, the probabilities that the POS tagger computed for that particular sequence of labels. A higher aggregate probablity == higher confidence in the predicted sequence of POS tags. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdaa423",
   "metadata": {},
   "source": [
    "## Conditional Random Fields (CRF) \n",
    "\n",
    "This sequence to sequence model requires feature engineering. It is set with features listed in https://towardsdatascience.com/pos-tagging-using-crfs-ea430c5fb78b#1c6a. Feel free to edit the features to see which combination gives the best results. \n",
    "\n",
    "**Log any features you add or change in the `_log.txt` file!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'len(word)': len(word),\n",
    "        'word[:4]': word[:4],\n",
    "        'word[:3]': word[:3],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[-4:]': word[-4:],\n",
    "        'word.lower()': word.lower(),\n",
    "        'word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word.lower()),\n",
    "        'word.ispunctuation': (word in string.punctuation),\n",
    "        'word.isdigit()': word.isdigit()}\n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word': word1,\n",
    "            '-1:len(word)': len(word1),\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word1.lower()),\n",
    "            '-1:word[:3]': word1[:3],\n",
    "            '-1:word[:2]': word1[:2],\n",
    "            '-1:word[-3:]': word1[-3:],\n",
    "            '-1:word[-2:]': word1[-2:],\n",
    "            '-1:word.isdigit()': word1.isdigit(),\n",
    "            '-1:word.ispunctuation': (word1 in string.punctuation)})     \n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i > 1:\n",
    "        word2 = sent[i-2][0]\n",
    "        features.update({\n",
    "            '-2:word': word2,\n",
    "            '-2:len(word)': len(word2),\n",
    "            '-2:word.lower()': word2.lower(),\n",
    "            '-2:word[:3]': word2[:3],\n",
    "            '-2:word[:2]': word2[:2],\n",
    "            '-2:word[-3:]': word2[-3:],\n",
    "            '-2:word[-2:]': word2[-2:],\n",
    "            '-2:word.isdigit()': word2.isdigit(),\n",
    "            '-2:word.ispunctuation': (word2 in string.punctuation),\n",
    "        })\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word': word1,\n",
    "            '+1:len(word)': len(word1),\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word[:3]': word1[:3],\n",
    "            '+1:word[:2]': word1[:2],\n",
    "            '+1:word[-3:]': word1[-3:],\n",
    "            '+1:word[-2:]': word1[-2:],\n",
    "            '+1:word.isdigit()': word1.isdigit(),\n",
    "            '+1:word.ispunctuation': (word1 in string.punctuation),\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        features['EOS'] = True    \n",
    "    \n",
    "    if i < len(sent) - 2:\n",
    "        word2 = sent[i+2][0]\n",
    "        features.update({\n",
    "            '+2:word': word2,\n",
    "            '+2:len(word)': len(word2),\n",
    "            '+2:word.lower()': word2.lower(),\n",
    "            '+2:word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', word2.lower()),\n",
    "            '+2:word[:3]': word2[:3],\n",
    "            '+2:word[:2]': word2[:2],\n",
    "            '+2:word[-3:]': word2[-3:],\n",
    "            '+2:word[-2:]': word2[-2:],\n",
    "            '+2:word.isdigit()': word2.isdigit(),\n",
    "            '+2:word.ispunctuation': (word2 in string.punctuation),\n",
    "        })\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words and labels, feed words to feature extractor\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [word[1] for word in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word[0] for word in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e80c5e1",
   "metadata": {},
   "source": [
    "### Main Function for CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8151580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainCRF():\n",
    "    \n",
    "    train, test, to_predict, tags = prepare('CRF')\n",
    "    \n",
    "    # extracting features from all the sentences\n",
    "    train_ftrs = [sent2features(s) for s in train]\n",
    "    train_tags = [sent2labels(s) for s in train]\n",
    "\n",
    "    test_ftrs = [sent2features(s) for s in test]\n",
    "    test_tags = [sent2labels(s) for s in test]\n",
    "    \n",
    "    to_predict_ftrs = [sent2features(s) for s in to_predict]\n",
    "    to_predict_words = [sent2tokens(s) for s in to_predict]\n",
    "    \n",
    "    # training parameters\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm = 'lbfgs',\n",
    "        c1 = 0.25,\n",
    "        c2 = 0.3,\n",
    "        max_iterations = 100,\n",
    "        all_possible_transitions=True)\n",
    "    #train\n",
    "    crf.fit(train_ftrs, train_tags)\n",
    "    # save model\n",
    "    with open(MODELDIR+TEAMCODE+TASK+'CRF'+ITERATION+'-model.pkl','wb') as f:\n",
    "        pickle.dump(crf,f)\n",
    "    \n",
    "    # testing\n",
    "    test_output = crf.predict(test_ftrs)\n",
    "    # get test reports\n",
    "    matrix = cm(test_tags, test_output, tags, 'CRF')\n",
    "    logResults(test_tags, test_output, matrix, 'CRF')\n",
    "    \n",
    "    # predict\n",
    "    predicted_labels = crf.predict(to_predict_ftrs)\n",
    "    predicted_sequences = [list(zip(to_predict_words[i], predicted_labels[i])) for i in range(len(predicted_labels))]\n",
    "    # get confidence score\n",
    "    all_probs = crf.predict_marginals(to_predict_ftrs)\n",
    "    confidences = []\n",
    "    for s,sent in enumerate(predicted_sequences):\n",
    "        confidences.append(sum(all_probs[s][i][wordpair[1]] for i,wordpair in enumerate(sent))/len(sent))\n",
    "    printPredictions(confidences, predicted_sequences, 'CRF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23dc359",
   "metadata": {},
   "source": [
    "# Run Code (all cells below)\n",
    "\n",
    "During the Machine-in-the-Loop activity help team members through these steps.\n",
    "\n",
    "    1) Form teams\n",
    "    2) Analyse Errors:\n",
    "        -- Examine evaluation metrics and confusion matrix in the display or the `.results' file.\n",
    "    3) Preprocess training data and/or CRFcustom code to improve Previous Model:\n",
    "        -- Make any changes to train data and unannotated file\n",
    "        -- Change .train and .predict filenames.\n",
    "        -- Code any customized model features in custom_word2features()\n",
    "        -- Make note of changes or other comments.\n",
    "    4) Adjust and prepare to run code that trains new model:\n",
    "        -- Update variables.\n",
    "        -- Add note of changes or other comments in `NOTES` variable.\n",
    "    5) Train, test, predict:\n",
    "        -- Run the `main` function for chosen model. \n",
    "        -- Eyeball log file. Edit notes\n",
    "        -- Eyeball files in `reports/` folder\n",
    "        -- Debug and rerun code if something seems off in those files.\n",
    "    6) Rinse and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b20975",
   "metadata": {},
   "source": [
    "#### IMPORTANT Notes about Data and Files\n",
    "\n",
    "- The three data sets are stored in files (with extensions: `.train`, `.test`, `.predict`) for easy printing and sharing and editing. Workshop participants can make changes to these data files, except the test data. Changes should be changed in new files with **the same file extensions!** These changed data will be used to train a new (and better?) POS tagger. \n",
    "\n",
    "- A `_log.txt` file will be written in the `reports/` folder before every training iteration. It contains the number of training tokens, the list and number of each POS tag in the training data.  **Please document any changes made since the last iteration which are not tracked with this code (e.g. \"30 tags were corrected in the training data\")**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e527c",
   "metadata": {},
   "source": [
    "##  Update Variables, Filenames and Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' File naming key: <teamcode>_<task><modelname><iter#>'''\n",
    "\n",
    "#UPDATE THESE\n",
    "TEAMCODE = 'tau' \n",
    "TASK = '_pos' \n",
    "PREVITERATION = '' # previous iteration #; predictions from prev iter.\n",
    "ITERATION = '0'    # current iteration #; used for edited train/test files\n",
    "NOTES = ''\n",
    "\n",
    "# Update these the first time only\n",
    "DATADIR = r'./Alberta/'+TEAMCODE+'/data/'\n",
    "REPORTDIR = r'./Alberta/'+TEAMCODE+'/reports/'\n",
    "MODELDIR = r'./Alberta/'+TEAMCODE+'/models/'\n",
    "\n",
    "SEPARATOR = '|'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474aeb86",
   "metadata": {},
   "source": [
    "## Train, Test, Predict CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4ad23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mainCRF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
